\section{C++ Implementation}

We have provided a numerical implementation of Gaussian Mixture Models in the C++ language
as part of recent releases of the Armadillo C++ linear algebra library~\cite{Armadillo_JOSS_2016}.
The implementation contains parallelised versions of the Expectation Maximisation (EM) and {\it k}-means training algorithms,
which can considerably speed up training.
Parallelisation is achieved through refactoring the original EM and {\it k}-means algorithms
into a MapReduce-like framework~\cite{MapReduce_2004} and employing OpenMP compiler directives~\cite{OpenMP_2007}.


\subsection{Numerical Overflows and Underflows}

Due to the necessarily limited precision of numerical floating point representations~\cite{TODO},

TODO: reformulation with log\_exp\_add




\subsection{User Accessible Classes and Functions}

There are two user accessible classes within the {\it arma} namespace:
{\it gmm\_diag} and {\it fgmm\_diag}.
The former uses double precision floating point values, while the latter uses single precision floating point values.

For an instance of the double precision {\it gmm\_diag} class named as {\bf M},
its member functions and variables are listed below.
All vectors and matrices refer to corresponding objects from the Armadillo library;
scalars have the type {\it double},
matrices have the type {\it mat},
column vectors have the type {\it vec},
row vectors have the type {\it rowvec},
row vectors of unsigned integers have the type {\it urowvec},
and indices have the type {\it uword} (representing an unsigned integer).
When using the single precision {\it fgmm\_diag} class,
all vector and matrix types have the {\it f} prefix (for example, {\it fmat}),
while scalars have the type {\it float}.
The word ``heft'' is used as a shorter version of ``weight'', while keeping the same meaning with the context of GMMs.
Figure~\ref{fig:example_usage} contain a complete C++ program which demonstrates usage of the {\it gmm\_diag} class.

\begin{small}
\begin{itemize}

\item
{\bf M.log\_p(V)}\\
return a scalar (double precision floating point value) representing the log-likelihood of column vector {\bf V}

\item
{\bf M.log\_p(V, g)}\\
return a scalar (double precision floating point value) representing the log-likelihood of column vector {\bf V},
according to Gaussian with index {\bf g} (specified as an unsigned integer of type {\it uword})

\item
{\bf M.log\_p(X)}\\
return a row vector (of type {\it rowvec}) containing log-likelihoods of each column vector in matrix {\bf X}


\item
{\bf M.log\_p(X, g)}\\
return a row vector containing log-likelihoods of each column vector in matrix {\bf X},
according to Gaussian with index {\bf g}  (specified as an unsigned integer of type {\it uword})

\item
{\bf M.avg\_log\_p(X)}\\
return a scalar (double precision floating point value) representing the average log-likelihood of all column vectors in matrix {\bf X}

\item
{\bf M.avg\_log\_p(X, g)}\\
return a scalar (double precision floating point value) representing the average log-likelihood of all column vectors in matrix {\bf X},
according to Gaussian with index {\bf g}  (specified as an unsigned integer of type {\it uword})

\item
{\bf M.assign(V, dist\_mode)}\\
return an unsigned integer (of type {\it uword}) representing the index of the closest mean (or Gaussian) to vector {\bf V};\\
parameter {\bf dist\_mode} is one of:

\begin{tabular}{ll}
{\bf eucl\_dist} & Euclidean distance (takes only means into account) \\
{\bf prob\_dist} & probabilistic ``distance'', defined as the inverse likelihood\\
                 & (takes into account means, covariances and hefts)
\end{tabular}

\item
{\bf M.assign(X, dist\_mode)}\\
return a row vector of unsigned integers (of type {\it urowvec}) containing the indices of the closest means (or Gaussians) to each column vector in matrix {\bf X};
parameter {\bf dist\_mode} is either {\bf eucl\_dist} or {\bf prob\_dist}, as per the {\bf .assign()} function above

\item
{\bf M.raw\_hist(X, dist\_mode)}\\
return a row vector of unsigned integers (of type {\it urowvec}) representing the raw histogram of counts;
each entry is the number of counts corresponding to a Gaussian;
each count is the number times the corresponding Gaussian was the closest to each column vector in matrix {\bf X};
parameter {\bf dist\_mode} is either {\bf eucl\_dist} or {\bf prob\_dist}, as per the {\bf .assign()} function above

\item
{\bf M.norm\_hist(X, dist\_mode)}\\
return a row vector containing normalised counts; the vector sums to one;
parameter {\bf dist\_mode} is either {\bf eucl\_dist} or {\bf prob\_dist}, as per the {\bf .assign()} function above

\item
{\bf M.generate()}\\
return a column vector representing a random sample generated according to the model's parameters

\item
{\bf M.generate(N)}\\
return a matrix containing {\bf N} column vectors, with each vector representing a random sample generated according to the model's parameters

\item
{\bf M.save(filename)}\\
save the model to the given filename

\item
{\bf M.load(filename)}\\
load the model from the given filename

\item
{\bf M.n\_gaus()}\\
return an unsigned integer (of type {\it uword}) containing the number of means/Gaussians in the model

\item
{\bf M.n\_dims()}\\
return an unsigned integer (of type {\it uword}) containing the dimensionality of the means/Gaussians in the model

\item
{\bf M.reset(n\_dims, n\_gaus)}\\
set the model to have dimensionality {\bf n\_dims}, with {\bf n\_gaus} number of Gaussians, specified as unsigned integers of type {\it uword};
all the means are set to zero, all diagonal covariances are set to one, and all the hefts (weights) are set to be uniform

\item
{\bf M.means}\\
read-only matrix containing the means (centroids), stored as column vectors

\item
{\bf M.dcovs}\\
read-only matrix containing the diagonal covariances, with the set of diagonal covariances for each Gaussian stored as a column vector

\item
{\bf M.hefts}\\
read-only row vector containing the hefts (weights)

\item
{\bf M.set\_means(X)}\\
set the means (centroids) to be as specified in matrix {\bf X}, with each mean (centroid) stored as a column vector;
the number of means and their dimensionality must match the existing model

\item
{\bf M.set\_dcovs(X)}\\
set the diagonal covariances to be as specified in matrix {\bf X}, with the set of diagonal covariances for each Gaussian stored as a column vector;
the number of diagonal covariance vectors and their dimensionality must match the existing model

\item
{\bf M.set\_hefts(V)}\\
set the hefts (weights) of the model to be as specified in row vector {\bf V};
the number of hefts must match the existing model

\item
{\bf M.set\_params(means, dcovs, hefts)}\\
set all the parameters at the same time, using matrices denoted as {\bf means} and {\bf dcovs} as well as the the row vector denoted as {\bf hefts};
the layout of the matrices is as per the {\bf .set\_means()} and {\bf .set\_dcovs()} functions above;
the number of Gaussians and dimensionality can be different from the existing model

\item
{\bf M.learn(data, n\_gaus, dist\_mode, seed\_mode, km\_iter, em\_iter, var\_floor, print\_mode)}\\
learn the model parameters via the {\it k}-means and/or EM algorithms,
and return a boolean value, with {\it true} indicating success, and {\it false} indicating failure;
the parameters have the following meanings:

\begin{enumerate}[{$\cdot$}]
\item
{\bf data}\\
matrix containing training samples; each sample is stored as a column vector

\item
{\bf n\_gaus}\\
set the number of Gaussians to {\bf n\_gaus}

\item
{\bf dist\_mode}\\
specifies the distance used during the seeding of initial means and {\it k}-means clustering:

\begin{tabular}{ll}
{\bf eucl\_dist} & Euclidean distance\\
{\bf maha\_dist} & Mahalanobis distance, which uses a global diagonal covariance matrix\\
                 & estimated from the given training samples
\end{tabular}

\item
{\bf seed\_mode}\\
specifies how the initial means are seeded prior to running {\it k}-means and/or EM algorithms:

\begin{tabular}{ll}
{\bf keep\_existing} & keep the existing model (do not modify the means, covariances and hefts) \\
{\bf static\_subset} & a subset of the training samples (repeatable) \\
{\bf random\_subset} & a subset of the training samples (random) \\
{\bf static\_spread} & a maximally spread subset of training samples (repeatable) \\
{\bf random\_spread} & a maximally spread subset of training samples (random start)
\end{tabular}

\item
{\bf km\_iter}\\
the maximum number of iterations of the {\it k}-means algorithm

\item
{\bf em\_iter}\\
the maximum number of iterations of the EM algorithm

\item
{\bf var\_floor}\\
the variance floor (smallest allowed value) for the diagonal covariances

\item
{\bf print\_mode}\\
boolean value (either {\it true} or {\it false}) which enables/disables the printing of progress during the {\it k}-means and EM algorithms 

\end{enumerate}

Note that seeding the initial means with {\bf static\_spread} and {\bf random\_spread}
can be more time consuming than with {\bf static\_subset} and {\bf random\_subset}.
These seed modes are inspired by the so-called {\it k-means++} approach~\cite{Arthur_2007}, with the aim to improve clustering quality.

\end{itemize}
\end{small}



\begin{figure}
\hrule
\vspace{0.5ex}
\begin{Verbatim}[fontsize=\footnotesize,fontseries=b]
#include <armadillo>

using namespace arma;

int main()
  {
  // create synthetic data with 2 Gaussians
  
  uword d = 5;       // dimensionality
  uword N = 10000;   // number of samples (vectors)
  
  mat data(d, N, fill::zeros);
  
  vec mean0 = linspace<vec>(1,d,d);
  vec mean1 = mean0 + 2;
  
  uword i = 0;
  
  while(i < N)
    {
    if(i < N)  { data.col(i) = mean0 + randn<vec>(d); ++i; }
    if(i < N)  { data.col(i) = mean0 + randn<vec>(d); ++i; }
    if(i < N)  { data.col(i) = mean1 + randn<vec>(d); ++i; }
    }
  
  // model the data as a GMM with 2 Gaussians
  
  gmm_diag model;
  
  bool status = model.learn(data, 2, maha_dist, random_subset, 10, 5, 1e-10, true);
  
  if(status == false)  { cout << "learning failed" << endl; }
  
  model.means.print("means:");
  
  double overall_likelihood = model.avg_log_p(data);
  
  rowvec     set_likelihood = model.log_p( data.cols(0,9) );
  double  scalar_likelihood = model.log_p( data.col(0)    );
  
  uword   gaus_id  = model.assign( data.col(0),    eucl_dist );
  urowvec gaus_ids = model.assign( data.cols(0,9), prob_dist );
  
  urowvec hist1 = model.raw_hist (data, prob_dist);
   rowvec hist2 = model.norm_hist(data, eucl_dist);
  
  model.save("my_model.gmm");
  
  mat dcovs2 = model.dcovs + 0.1;
  
  model.set_dcovs(dcovs2);
  
  return 0;
  }
\end{Verbatim}
\vspace{-1ex}
\hrule
\vspace{0.5ex}
\caption
  {
  An example C++ program which demonstrates usage of the {\it gmm\_diag} class.
  }
\label{fig:example_usage}
\end{figure}
