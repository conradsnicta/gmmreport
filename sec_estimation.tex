\section{Parameter Estimation via Expectation Maximisation}


\noindent
The overall likelihood for a set of samples, $X=\{\Vec{x}_i\}_{i=1}^{N_V}$,
is found using $p(X | \lambda) = \prod\nolimits_{i=1}^{N_V} p(\Vec{x}_i | \lambda)$.
A~parameter set $\lambda = \{ w_g, \Vec{\mu}_g, \Mat{\Sigma}_g \}_{g=1}^{N_G}$
that suitably models the underlying distribution of $X$ can be estimated using a particular instance of the Expectation Maximisation (EM) algorithm~\cite{Dempster77, McLachlan-2008, Moon96, Redner84}.
%The full derivation of the EM algorithm for GMM parameter estimation is beyond the scope of this chapter -
%the reader is encouraged to refer to~\cite{Bilmes98,Redner84,Reynolds93} or see Appendix~\ref{app:em_algorithm}.
As its name suggests, the EM algorithm is comprised of iterating two steps: the {\it expectation} step, followed by the {\it maximisation} step.
GMM parameters generated by the previous iteration ($\lambda^{\mbox{\footnotesize old}}$) are used
by the current iteration to generate a new set of parameters ($\lambda^{\mbox{\footnotesize new}}$), such that:
%
\begin{eqnarray}
	p(X|\lambda^{\mbox{\footnotesize new}}) \geq p(X|\lambda^{\mbox{\footnotesize old}})
\end{eqnarray}
%
The process is repeated until convergence or until the increase in the overall likelihood after each iteration falls below a pre-defined threshold.
The initial estimate is typically provided by the $k$-means clustering algorithm~\cite{Duda01}.
A direct implementation of the EM algorithm, specific to GMMs, has one iteration succinctly expressed as follows:
%
\begin{eqnarray}
%
\mbox{for} ~ g=1, \cdots,N_G: ~~ \mbox{for} ~i=1, \cdots, N_V:  ~~ l_{g,i} = \frac{m_g ~ {{\mathcal{N}}}( \Vec{x}_i | \Vec{\mu}_g, \Mat{\Sigma}_g )}
														{\sum\nolimits_{n=1}^{N_G} m_n ~ {{\mathcal{N}}}( \Vec{x}_i | \Vec{\mu}_n, \Mat{\Sigma}_n )}
%
\end{eqnarray}
%
\begin{eqnarray}
%
\mbox{for} ~~ g  =  1, \cdots, N_G:  \nonumber \\
	L_g & = & \sum\nolimits_{i=1}^{N_V} l_{g,i} \label{eqn:L_k} \\
	\widehat{m}_g & = & \frac{L_g}{N_V} \label{eqn:m_k} \\
	\widehat{\Vec{\mu}}_g & = & \frac{1}{L_g} \sum\nolimits_{i=1}^{N_V} \Vec{x}_i ~ l_{g,i}  \label{eqn:em_mean} \\
	\widehat{\Mat{\Sigma}}_g & = & \frac{1}{L_g} \sum\nolimits_{i=1}^{N_V} (\Vec{x}_i - \widehat{\Vec{\mu}}_g)(\Vec{x}_i - \widehat{\Vec{\mu}}_g)^T l_{g,i} \label{eqn:em_covA} \\
	& = &  \frac{1}{L_g} \left[ \sum\nolimits_{i=1}^{N_V} \Vec{x}_i \Vec{x}_i^T l_{g,i} \right] - \widehat{\Vec{\mu}}_g \widehat{\Vec{\mu}}_g^T \label{eqn:em_covB} \\
\end{eqnarray}

\noindent
Once all $\widehat{m}_g, \widehat{\Vec{\mu}}_g, \widehat{\Mat{\Sigma}}_g$ are found, the parameters are updated:

\begin{equation}
\left\{ m_g, \Vec{\mu}_g, \Mat{\Sigma}_g \right\}_{g=1}^{N_G} ~ = ~ \left\{ \widehat{m}_g, \widehat{\Vec{\mu}}_g, \widehat{\Mat{\Sigma}}_g \right\}_{g=1}^{N_G} 
\end{equation}

\noindent In the above, $l_{g,i} \in [0,1]$ is the {a-posteriori} probability of Gaussian $g$ given $\Vec{x}_i$ and current parameters.
Thus the estimates $\widehat{\Vec{\mu}}_g$ and $\widehat{\Mat{\Sigma}}_g$ are weighted versions of the
sample mean and sample covariance, respectively.
For a derivation of the EM algorithm for GMM parameters, the reader is directed to~\cite{Bilmes98, Redner84} or Appendix~\ref{app:em_algorithm}.

Overall, the algorithm is a hill climbing procedure for maximising $p(X | \lambda)$.
While there are no guarantees that it will reach a global maximum, it is guaranteed to monotonically converge to a saddle point or a local maximum~\cite{Dempster77,Duda01,Mitchell97}.
The above implementation can also be interpreted as an unsupervised probabilistic clustering procedure,
with $N_G$ being the assumed number of clusters.

The initial estimate of $\lambda$ is provided via the {\it k}-means clustering algorithm~\cite{Duda01},
described below.

%
% k-means
%

\subsubsection{$k$-means}
\label{sec:ch_gmm_kmeans}

We use the Kronecker delta function, $\delta(\cdot,\cdot)$,
which has a value of $1$ if its two arguments match and $0$ if they do not.
We also use a {\it randi}$(min,max)$ function, 
which generates a uniformly distributed random integer value in the $[min,max]$ interval.
The essence of the $k$-means \index{k-means} algorithm is described using the following pseudo-code:

\begin{small}
\begin{tabbing}
01: {\bf for} $g=1, ~\cdots, ~N_G$ \\
02: ~~ $\Vec{\mu}_g = \Vec{x}_{ \mbox{\it randi}(1, N_V) } $ ~~~ {\small // randomly select initial means} \\
03: {\bf endfor} \\
04: {\it loop} = $1$ \\
05: {\it endloop} = $10$ ~~~ ~~~ ~~~ {\small // empirically chosen termination condition (see Section~\ref{sec:impl_kmeans})} \\
06: {\it finished} = FALSE \\
07: {\bf while} {\it finished} $\neq$ TRUE \\
08: ~~ {\bf for} $i=1, ~\cdots, ~N_V$ \\
09: ~~ ~~ \( y_i = {\displaystyle \arg \min_{g=1, \cdots, N_G}} || \Vec{\mu}_g - \Vec{x}_i || \) ~~~ {\small // label each vector as belonging to its closest mean} \\
10: ~~ {\bf endfor} \\
11: ~~ {\bf for} $g=1, ~\cdots, ~N_G$ \\
12: ~~ ~~ $n_g = \sum\nolimits_{i=1}^{N_V} \delta(y_i, g) $  ~~~ ~~~ ~~~ ~~~ {\small // count the number of vectors assigned to each mean} \\
13: ~~ ~~ $\widehat{\Vec{\mu}}_g = \frac{1}{n_g} \sum\nolimits_{i=1}^{N_V} \Vec{x}_i \delta(y_i, g) $ ~~~ ~~~ {\small // find the new mean using vectors assigned to the old mean} \\
14: ~~ {\bf endfor} \\
15: ~~ {\it same} = TRUE \\
16: ~~ {\bf for} $g=1, ~\cdots, ~N_G$ \\
17: ~~ ~~ {\bf if} $\widehat{\Vec{\mu}}_g \neq \Vec{\mu}_g $  ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~~~ {\small // see if the means have changed since the last iteration} \\
18: ~~ ~~ ~~ {\it same} = FALSE \\
19: ~~ ~~ {\bf endif} \\
20: ~~ {\bf endfor} \\
21: ~~ {\it loop} = {\it loop} + 1 \\
22: ~~ {\bf if} ({\it same} == TRUE) {\bf or} ({\it loop} $>$ {\it endloop})  \\
23: ~~ ~~ {\it finished} = TRUE  ~~~~~~~~~~~~~~~~~~ {\small // finish if the means haven't changed since the last iteration} \\
24: ~~ {\bf endif} ~~~ ~~ ~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ {\small // or the maximum number of iterations has been reached} \\
25: ~~ {\bf for} $g=1, ~\cdots, ~N_G$ \\
26: ~~ ~~ $\Vec{\mu}_g = \widehat{\Vec{\mu}}_g$ ~~~ {\small // update the mean vectors} \\
27: ~~ {\bf endfor} \\
28: {\bf endwhile}
\end{tabbing}
\end{small}

In practise it is possible that while iterating at least one of the means has no vectors assigned to it,
becoming a ``dead'' mean.
This can be due to an unfortunate starting point and/or a lack of data.
As such, an additional heuristic is required to attempt to resolve this situation.
One example of resurrecting a ``dead'' mean is to make it equal to one of the vectors
that has been assigned to the most ``popular'' mean,
where the most ``popular'' mean is the mean that currently has the most vectors assigned to it.

Once the estimated means, $\{ \Vec{\mu}_g \}_{g=1}^{N_G}$, have been found,
the initial weights, $\{ m_g \}_{g=1}^{N_G}$, and initial covariance matrices, $\{ {\Mat{\Sigma}_g} \}_{g=1}^{N_G}$,
are estimated as follows:
%
\begin{eqnarray}
m_g & = & \frac{n_g}{N_V} \\
{\mbox {\boldmath $\Sigma$}_g} & = & \frac{1}{n_g} \sum\nolimits_{i=1}^{N_V} (\Vec{x}_i - \Vec{\mu}_g)(\Vec{x}_i - \Vec{\mu}_g)^T \delta(y_i, g)
\label{eqn:k-means_cov}
\end{eqnarray}

\noindent
where $n_g$ is defined on line 12 of the pseudo-code and $y_i$ on line 9.

The $k$-means algorithm can be interpreted as a special case of the EM algorithm for GMMs.
Instead of each vector being assigned a set probabilities as to which Gaussian it belongs to,
each vector is assigned to only one Gaussian.
Furthermore, it is assumed that the covariance matrix of each Gaussian is diagonal.
We also note that the $k$-means algorithm can also be implemented in a somewhat different manner, 
for example the ``splitting'' LBG algorithm~\cite{Linde80}.

