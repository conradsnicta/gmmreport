\section{Parameter Estimation via Expectation Maximisation}


\noindent
The overall likelihood for a set of samples, $X=\{\Vec{x}_i\}_{i=1}^{N_V}$,
is found using $p(X | \lambda) = \prod\nolimits_{i=1}^{N_V} p(\Vec{x}_i | \lambda)$.
A~parameter set $\lambda = \{ w_g, \Vec{\mu}_g, \Mat{\Sigma}_g \}_{g=1}^{N_G}$
that suitably models the underlying distribution of $X$ can be estimated using a particular instance of the Expectation Maximisation (EM) algorithm~\cite{Dempster77, McLachlan-2008, Moon96, Redner84}.
As its name suggests, the EM algorithm is comprised of iterating two steps: the {\it expectation} step, followed by the {\it maximisation} step.
GMM parameters generated by the previous iteration ($\lambda^{\mbox{\footnotesize old}}$) are used
by the current iteration to generate a new set of parameters ($\lambda^{\mbox{\footnotesize new}}$), such that:
%
\begin{eqnarray}
	p(X|\lambda^{\mbox{\footnotesize new}}) \geq p(X|\lambda^{\mbox{\footnotesize old}})
\end{eqnarray}
%
A direct implementation of the EM algorithm, specific to GMMs, has one iteration succinctly expressed as follows:
%
\begin{eqnarray}
  \mbox{for} ~g=1, \cdots, N_G: \nonumber \\
  \mbox{for} ~i=1, \cdots, N_V: \nonumber \\
  l_{g,i} & = & \frac{w_g ~ {{\mathcal{N}}}( \Vec{x}_i | \Vec{\mu}_g, \Mat{\Sigma}_g )}
  {\sum\nolimits_{n=1}^{N_G} w_n ~ {{\mathcal{N}}}( \Vec{x}_i | \Vec{\mu}_n, \Mat{\Sigma}_n )} \label{eqn:aposteriori}\\
\mbox{for} ~~ g  =  1, \cdots, N_G:  \nonumber \\
	L_g & = & \sum\nolimits_{i=1}^{N_V} l_{g,i} \label{eqn:L_k} \\
	\widehat{w}_g & = & \frac{L_g}{N_V} \label{eqn:m_k} \\
	\widehat{\Vec{\mu}}_g & = & \frac{1}{L_g} \sum\nolimits_{i=1}^{N_V} \Vec{x}_i ~ l_{g,i}  \label{eqn:em_mean} \\
	\widehat{\Mat{\Sigma}}_g & = & \frac{1}{L_g} \sum\nolimits_{i=1}^{N_V} (\Vec{x}_i - \widehat{\Vec{\mu}}_g)(\Vec{x}_i - \widehat{\Vec{\mu}}_g)^T l_{g,i} \label{eqn:em_covA} \\
	& = &  \frac{1}{L_g} \left[ \sum\nolimits_{i=1}^{N_V} \Vec{x}_i \Vec{x}_i^T l_{g,i} \right] - \widehat{\Vec{\mu}}_g \widehat{\Vec{\mu}}_g^T \label{eqn:em_covB} \\
\end{eqnarray}

\noindent
Once all $\widehat{w}_g, \widehat{\Vec{\mu}}_g, \widehat{\Mat{\Sigma}}_g$ are found, the parameters are updated,
ie., $\left\{ w_g, \Vec{\mu}_g, \Mat{\Sigma}_g \right\}_{g=1}^{N_G} ~ = ~ \left\{ \widehat{w}_g, \widehat{\Vec{\mu}}_g, \widehat{\Mat{\Sigma}}_g \right\}_{g=1}^{N_G}$,
and the iteration starts anew.
The process is typically repeated until the number of iterations has reached a pre-defined number,
and/or the increase in the overall likelihood after each iteration falls below a pre-defined threshold.

\noindent In Eqn.~(\ref{eqn:aposteriori}), $l_{g,i} \in [0,1]$ is the {a-posteriori} probability of Gaussian $g$ given $\Vec{x}_i$ and current parameters.
Thus the estimates $\widehat{\Vec{\mu}}_g$ and $\widehat{\Mat{\Sigma}}_g$ are weighted versions of the
sample mean and sample covariance, respectively.
Overall, the algorithm is a hill climbing procedure for maximising $p(X | \lambda)$.
While there are no guarantees that it will reach a global maximum, it is guaranteed to monotonically converge to a saddle point or a local maximum~\cite{Dempster77,Duda01,Mitchell97}.
The above implementation can also be interpreted as an unsupervised probabilistic clustering procedure,
with $N_G$ being the assumed number of clusters.
For a derivation of the EM algorithm for GMM parameters, the reader is directed to~\cite{Bilmes98, Redner84} or Appendix~\ref{app:em_algorithm}.

\subsection{Initial Estimate}
\label{sec:initial_estimate}

As a starting point, the initial means can be set to randomly selected training vectors,
the initial covariance matrices can be set equal to identity matrices, 
and the initial weights can be uniform.
However, in a computational implementation, the $\exp()$ function as well as the matrix inverse in Eqn.~(\ref{eqn:gaussian}) are typically quite time consuming.
In order to speed up training, the initial estimate of $\lambda$ is typically provided via the {\it k}-means clustering algorithm~\cite{Duda01}.

The $k$-means algorithm can be interpreted as a simplified version (or special case) of the EM algorithm for GMMs.
Instead of each vector being assigned a set probabilities as to which Gaussian it belongs to,
each vector is assigned to only one Gaussian.
Furthermore, it is typically assumed that the covariance matrix of each Gaussian is non-informative, diagonal, and/or shared across all Gaussians.

For the case of non-informative covariance matrices, the EM algorithm essentially reduces to an iterative procedure comprised of two steps:
(i)~calculating the Euclidean distance between each vector and each mean,
and
(ii)~calculating the new version of each mean as the average of vectors which were found to be the closest to the previous version of the corresponding mean.
The pseudo-code for a direct implementation of a baseline $k$-means algorithm is shown in Figure~\ref{fig:kmeans_pseudocode}.

\begin{figure}[!b]
\hrule
\begin{small}
\begin{tabbing}
01: {\bf for} $g=1, ~\cdots, ~N_G$ \\
02: ~~ $\Vec{\mu}_g = \Vec{x}_{ \mbox{\it randi}(1, N_V) } $ ~~~ {\small // randomly select initial means} \\
03: {\bf endfor} \\
04: {\it iteration} = $1$ \\
05: {\it final\_iteration} = $10$ ~~~ ~~~ ~~~ {\small // empirically chosen termination condition} \\
06: {\it finished} = FALSE \\
07: {\bf while} {\it finished} $\neq$ TRUE \\
08: ~~ {\bf for} $i=1, ~\cdots, ~N_V$ \\
09: ~~ ~~ \( y_i = {\displaystyle \arg \min_{g=1, \cdots, N_G}} \operatorname{\mbox{\it dist}}(\Vec{\mu}_g, \Vec{x}_i) \) ~~~ {\small // label each vector as belonging to its closest mean} \\
10: ~~ {\bf endfor} \\
11: ~~ {\bf for} $g=1, ~\cdots, ~N_G$ \\
12: ~~ ~~ $n_g = \sum\nolimits_{i=1}^{N_V} \delta(y_i, g) $  ~~~ ~~~ ~~~ ~~~ {\small // count the number of vectors assigned to each mean} \\
13: ~~ ~~ $\widehat{\Vec{\mu}}_g = \frac{1}{n_g} \sum\nolimits_{i=1}^{N_V} \Vec{x}_i \delta(y_i, g) $ ~~~ ~~~ {\small // find the new mean using vectors assigned to the old mean} \\
14: ~~ {\bf endfor} \\
15: ~~ {\it same} = TRUE \\
16: ~~ {\bf for} $g=1, ~\cdots, ~N_G$ \\
17: ~~ ~~ {\bf if} $\widehat{\Vec{\mu}}_g \neq \Vec{\mu}_g $  ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~~~ {\small // see if the means have changed since the last iteration} \\
18: ~~ ~~ ~~ {\it same} = FALSE \\
19: ~~ ~~ {\bf endif} \\
20: ~~ {\bf endfor} \\
21: ~~ {\it iteration} = {\it iteration} + 1 \\
22: ~~ {\bf if} ({\it same} == TRUE) {\bf or} ({\it iteration} $>$ {\it final\_iteration})  \\
23: ~~ ~~ {\it finished} = TRUE \\
24: ~~ {\bf endif} \\
25: ~~ {\bf for} $g=1, ~\cdots, ~N_G$ \\
26: ~~ ~~ $\Vec{\mu}_g = \widehat{\Vec{\mu}}_g$ ~~~ {\small // update the mean vectors} \\
27: ~~ {\bf endfor} \\
28: {\bf endwhile}
\end{tabbing}
\end{small}
\caption
  {
  \small
  Pseudo-code for a direct implementation of the $k$-means algorithm.
  Each training vector is expressed as $\Vec{x}_i$, where $i = 1, \cdots, N_V$, with $N_V$ indicating the number of vectors available for training.
  Each mean vector is expressed as $\Vec{\mu}_g$, where $g = 1, \cdots, N_G$, with $N_G$ indicating the number of required means (centroids).
  The~{\it randi}$(min,max)$ function generates a uniformly distributed random integer value in the $[min,max]$ interval.
  The~{\it dist}$(\Vec{x},\Vec{y})$ function calculates a distance between vectors $\Vec{x}$ and $\Vec{y}$, with a distance of zero indicating that the vectors are equal.
  The~$\delta(\cdot,\cdot)$ function (Kronecker delta) has a value of $1$ if its two arguments match and $0$ if they do not.
  }
\label{fig:kmeans_pseudocode}
\hrule
\end{figure}

Once the estimated means, $\{ \Vec{\mu}_g \}_{g=1}^{N_G}$, have been found,
the initial weights, $\{ w_g \}_{g=1}^{N_G}$, and initial covariance matrices, $\{ {\Mat{\Sigma}_g} \}_{g=1}^{N_G}$,
are estimated as follows:
%
\begin{eqnarray}
w_g & = & \frac{n_g}{N_V} \\
{\mbox {\boldmath $\Sigma$}_g} & = & \frac{1}{n_g} \sum\nolimits_{i=1}^{N_V} (\Vec{x}_i - \Vec{\mu}_g)(\Vec{x}_i - \Vec{\mu}_g)^T \delta(y_i, g)
\label{eqn:k-means_cov}
\end{eqnarray}
%
\noindent
where $n_g$ is defined on line 12 of the pseudo-code and $y_i$ on line 9.

We also note that the $k$-means algorithm can also be implemented in a somewhat different manner, 
for example the ``splitting'' LBG algorithm~\cite{Linde80}.

In practise it is possible that while iterating at least one of the means has no vectors assigned to it,
becoming a ``dead'' mean.
This can be due to an unfortunate starting point and/or a lack of data.
As such, an additional heuristic is required to attempt to resolve this situation.
One example of resurrecting a ``dead'' mean is to make it equal to one of the vectors
that has been assigned to the most ``popular'' mean,
where the most ``popular'' mean is the mean that currently has the most vectors assigned to it.

