\section{Introduction}

Modelling multivariate data through a convex mixture of Gaussians, also known as a Gaussian mixture model (GMM),
has many uses in fields such as signal processing, econometrics, pattern recognition, machine learning and computer vision.
Examples of applications include
multi-stage feature extraction for action recognition~\cite{Carvajal_2016a},
modelling of intermediate features derived from deep convolutional neural networks~\cite{Ge_ICIP_2015,LeCun_Nature_2015},
classification of human epithelial cell images~\cite{Wiliem_PR_2014},
implicit sparse coding for face recognition~\cite{Wong_2014},
speech-based identity verification~\cite{Reynolds_2000},
and probabilistic foreground estimation for surveillance systems~\cite{Reddy_2013}.
GMMs are also commonly used as the emission distribution for hidden Markov models~\cite{Bilmes98}.

In the GMM approach, a distribution of samples (vectors) is modelled as:
%
\begin{equation}
  p(\Vec{x} | \lambda) = \sum\nolimits_{g=1}^{N_G} w_g ~ {{\mathcal{N}}}( \Vec{x} | \Vec{\mu}_g, \Mat{\Sigma}_g )
  \label{eqn:gmm_prob}
\end{equation}%
%
where $\Vec{x}$ is a $D$-dimensional vector,
$w_g$ is the weight for component $g$ (with constraints $\sum\nolimits_{g=1}^{N_G} w_g = 1$, $w_g \geq 0$),
and
${{\mathcal{N}}}( \Vec{x} | \Vec{\mu}, \Mat{\Sigma})$ is a $D$-dimensional Gaussian density function with mean $\Vec{\mu}$ and covariance matrix $\Mat{\Sigma}$:
%
\begin{small}
\begin{equation}
  {{\mathcal{N}}}( \Vec{x} | \Vec{\mu}, \Mat{\Sigma} )  = 
  \frac{1}{ (2\pi)^{\frac{D}{2}} | \Mat{\Sigma}|^{\frac{1}{2}} }
  \exp \left[ -\frac{1}{2} (\Vec{x}-\Vec{\mu})^\top \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu}) \right]
  \label{eqn:gaussian}
\end{equation}%
\end{small}%
%
where $|\Mat{\Sigma}|$ and $\Mat{\Sigma}^{-1}$ denote the determinant and inverse of $\Mat{\Sigma}$, respectively,
while $\Vec{x}^\top$ denotes the transpose of $\Vec{x}$.
The full parameter set can be compactly stated as $\lambda = \{ w_g, \Vec{\mu}_g, \Mat{\Sigma}_g \}_{g=1}^{N_G}$,
where $N_G$ is the number of Gaussians.

Given a training dataset and a value for $N_G$,
the estimation of $\lambda$ is typically done through a
tailored instance of Expectation Maximisation (EM) algorithm~\cite{Dempster77, McLachlan-2008, Moon96, Redner84}.
The {\it k}-means algorithm~\cite{Bishop_2006,Duda01,Linde80} is also typically used for providing the initial estimate of $\lambda$ for the EM algorithm.
Choosing the optimal $N_G$ is data dependent and beyond the scope of this work; see~\cite{Hamerly_2003,Pelleg_2000} for example methods.

%The {\it k}-means and EM algorithms are computationally intensive.
Unfortunately, GMM parameter estimation via the EM algorithm is computationally intensive
and can suffer from numerical stability issues.
Given the ever growing sizes of datasets and the need for fast, robust and accurate modelling of such datasets,
we have provided an open source implementation of multi-threaded (parallelised) versions 
of the \mbox{{\it k}-means} and EM algorithms.
In addition, core functions are recast in order to considerably reduce the likelihood of numerical instability due to floating point underflows and overflows.
The implementation is provided as a user-friendly class in recent releases of the cross-platform Armadillo C++ linear algebra library~\cite{Armadillo_JOSS_2016}.
The library is licensed under the permissive Apache~2.0 license~\cite{Laurent_2008},
thereby allowing unencumbered use in commercial products.

We continue the paper as follows.
In Section~\ref{sec:param_em} we provide an overview of parameter estimation via the EM algorithm,
its reformulation for multi-threaded execution,
and approaches for improving numerical stability.
In Section~\ref{sec:param_km} we provide a summary of the {\it k}-means algorithm
along with approaches for improving its convergence and modelling accuracy.
The numerical implementation in C++ is overviewed in~Section~\ref{sec:implementation},
where we list and describe the user accessible functions.
In Section~\ref{sec:eval}
we provide a demonstration that the implementation can achieve a speedup of an order of magnitude on a recent 16 core machine,
as well as obtain higher modelling accuracy than a previously established publically accessible implementation.

% Parallelisation is achieved through refactoring the original EM and {\it k}-means algorithms
% into a MapReduce-like framework~\cite{MapReduce_2004} and employing OpenMP compiler directives~\cite{OpenMP_2007}.



