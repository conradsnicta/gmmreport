\section{\small Expectation Maximisation and Multi-Threading}
\label{sec:param_em}

\noindent
The overall likelihood for a set of samples, $X=\{\Vec{x}_i\}_{i=1}^{N_V}$,
is found using $p(X | \lambda) = \prod\nolimits_{i=1}^{N_V} p(\Vec{x}_i | \lambda)$.
A~parameter set $\lambda$ that suitably models the underlying distribution of $X$ can be estimated using a particular instance of the Expectation Maximisation (EM) algorithm~\cite{Dempster77, McLachlan-2008, Moon96, Redner84}.
As its name suggests, the EM algorithm is comprised of iterating two steps: the {\it expectation} step, followed by the {\it maximisation} step.
GMM parameters generated by the previous iteration~($\lambda^{\textrm{old}}$) are used
by the current iteration to generate a new set of parameters~($\lambda^{\textrm{new}}$), such that:
%
\begin{eqnarray}
	p(X|\lambda^{\textrm{new}}) \geq p(X|\lambda^{\textrm{old}})
\end{eqnarray}
%
In a direct implementation of the EM algorithm specific to GMMs,
the estimated versions of the parameters ($\widehat{w}_g$, $\widehat{\Vec{\mu}}_g$, $\widehat{\Mat{\Sigma}}_g$)
within one iteration are calculated as follows:
%
\begin{eqnarray}
  l_{g,i}                  & = & \frac{w_g ~ {{\mathcal{N}}}( \Vec{x}_i | \Vec{\mu}_g, \Mat{\Sigma}_g )}{\sum\nolimits_{k=1}^{N_G} w_k ~ {{\mathcal{N}}}( \Vec{x}_i | \Vec{\mu}_k, \Mat{\Sigma}_k )}, \label{eqn:aposteriori} \\
  L_g                      & = & \sum\nolimits_{i=1}^{N_V} l_{g,i}, \label{eqn:em_sum_lhood} \\
  \widehat{w}_g            & = & \frac{L_g}{N_V},  \label{eqn:em_weight} \\
  \widehat{\Vec{\mu}}_g    & = & \frac{1}{L_g} \sum\nolimits_{i=1}^{N_V} \Vec{x}_i ~ l_{g,i}  \label{eqn:em_mean}, \\
  \widehat{\Mat{\Sigma}}_g & = & \frac{1}{L_g} \sum\nolimits_{i=1}^{N_V} (\Vec{x}_i - \widehat{\Vec{\mu}}_g)(\Vec{x}_i - \widehat{\Vec{\mu}}_g)^\top l_{g,i}. \label{eqn:em_cov}
%                           & = &  \frac{1}{L_g} \left[ \sum\nolimits_{i=1}^{N_V} \Vec{x}_i \Vec{x}_i^\top l_{g,i} \right] - \widehat{\Vec{\mu}}_g \widehat{\Vec{\mu}}_g^\top.
\end{eqnarray}

\noindent
Once the estimated parameters for all Gaussians are found, the parameters are updated:

\begin{equation}
\left\{ w_g, \Vec{\mu}_g, \Mat{\Sigma}_g \right\}_{g=1}^{N_G} = \left\{ \widehat{w}_g, \widehat{\Vec{\mu}}_g, \widehat{\Mat{\Sigma}}_g \right\}_{g=1}^{N_G}
\end{equation}

\noindent and the iteration starts anew.
The process is typically repeated until the number of iterations has reached a pre-defined number,
and/or the increase in the overall likelihood after each iteration falls below a pre-defined threshold.

In Eqn.~(\ref{eqn:aposteriori}), $l_{g,i} \in [0,1]$ is the {a-posteriori} probability of Gaussian $g$ given $\Vec{x}_i$ and current parameters.
Thus the estimates $\widehat{\Vec{\mu}}_g$ and $\widehat{\Mat{\Sigma}}_g$ are weighted versions of the
sample mean and sample covariance, respectively.

Overall, the algorithm is a hill climbing procedure for maximising $p(X | \lambda)$.
While there are no guarantees that it will reach a global maximum, it is guaranteed to monotonically converge to a saddle point or a local maximum~\cite{Dempster77,Duda01,Mitchell97}.
The above implementation can also be interpreted as an unsupervised probabilistic clustering procedure,
with $N_G$ being the assumed number of clusters.
For a full derivation of the EM algorithm tailored to GMMs, the reader is directed to~\cite{Bilmes98,Redner84}. % or Appendix~A. %~\ref{app:em_algorithm}.


\subsection{Reformulation for Multi-Threaded Execution}
\label{sec:param_em_parallel}

The EM algorithm is quite computationally intensive.
This is in large part due to the use of the {\small $\exp(\cdot)$} function, which needs to be applied numerous times for each and every sample.
Fortunately, CPUs with a multitude of cores are now quite common and accessible, allowing for multi-threaded (parallel) execution.

One approach for parallelisation is the MapReduce framework~\cite{MapReduce_2004},
where data is split into chunks and farmed out to separate workers for processing (mapping).
The results are then collected and combined (reduced) to produce the final result.
Below we provide a reformulation of the EM algorithm into a MapReduce-like framework.

As Eqn.~(\ref{eqn:aposteriori}) can be executed independently for each sample,
the summations in Eqns.~(\ref{eqn:em_sum_lhood}) and (\ref{eqn:em_mean}) can be split into separate sets of summations,
where the summation in each set can be executed independently and in parallel with other sets.
To allow similar splitting of the summation for calculating covariance matrices,
Eqn.~(\ref{eqn:em_cov}) needs to be rewritten into the following form:
%
\begin{equation}
  \widehat{\Mat{\Sigma}}_g = \frac{1}{L_g} \left[ \sum\nolimits_{i=1}^{N_V} \Vec{x}_i \Vec{x}_i^\top l_{g,i} \right] - \widehat{\Vec{\mu}}_g \widehat{\Vec{\mu}}_g^\top.
\end{equation}

The multi-threaded estimation of the parameters can now be formally stated as follows.
Given $N_T$ threads, the training samples are split into $N_T$ chunks, with each chunk containing approximately the same amount of samples.
For thread with index $t \in [1,N_T]$, the start index of the samples is denoted by $i^{[t]}_{\textrm{start}}$,
while the end index is denoted by $i^{[t]}_{\textrm{end}}$.
For~each thread $t$ and Gaussian $g \in [1,N_G]$, accumulators $\widetilde{L}_g^{[t]}$, $\widetilde{\Vec{\mu}}_g^{[t]}$ and $\widetilde{\Mat{\Sigma}}_g^{[t]}$
are calculated as follows:
%
\begin{eqnarray}
  %\mbox{for} ~~ g = 1, \cdots, N_g:  \nonumber \\
  %
  \widetilde{L}_g^{[t]}            & = & \sum\nolimits_{j = i^{[t]}_{\textrm{start}}}^{i^{[t]}_{\textrm{end}}} l_{g,j},                             \\ % acc_norm_lhoods
  \widetilde{\Vec{\mu}}_g^{[t]}    & = & \sum\nolimits_{j = i^{[t]}_{\textrm{start}}}^{i^{[t]}_{\textrm{end}}} l_{g,j} ~ \Vec{x}_j,                 \\ % acc_mean
  %\widetilde{\Mat{\Sigma}}_g^{[t]} & = & \sum\nolimits_{j = i^{[t]}_{\textrm{start}}}^{i^{[t]}_{\textrm{end}}} l_{g,j} ~ \Vec{x}_j \odot \Vec{x}_j    % acc_dcov
  \widetilde{\Mat{\Sigma}}_g^{[t]} & = & \sum\nolimits_{j = i^{[t]}_{\textrm{start}}}^{i^{[t]}_{\textrm{end}}} l_{g,j} ~ \Vec{x}_j \Vec{x}_j^\top.    % acc_dcov
\end{eqnarray}%
%
where $l_{g,j}$ is defined in Eqn.~(\ref{eqn:aposteriori}).
%while $\odot$ represents the Schur product (element-wise multiplication).

Once the accumulators for all threads are calculated,
for each Gaussian $g$ the reduction operation combines them to form the estimates of $\widehat{\Vec{\mu}}_g$ and $\widehat{\Mat{\Sigma}}_g$ as follows:
%
\begin{eqnarray}
  L_g                      & = & \sum\nolimits_{t=1}^{N_T} \widetilde{L}_g^{[t]},                           \label{eqn:combined_L_g} \\
  %\widehat{w}_g            & = & \frac{L_g}{N_V},                                                                                   \\
  \widehat{\Vec{\mu}}_g    & = & \frac{1}{L_g} \sum\nolimits_{t=1}^{N_T} \widetilde{\Vec{\mu}}_g^{[t]},                              \\
  \widehat{\Mat{\Sigma}}_g & = & \frac{1}{L_g} \sum\nolimits_{t=1}^{N_T} \widetilde{\Mat{\Sigma}}_g^{[t]} - \widehat{\Vec{\mu}}_g \widehat{\Vec{\mu}}_g^\top.
\end{eqnarray}

\noindent
The estimation of $\widehat{w}_g$ is as per Eqn.~(\ref{eqn:em_weight}), but using $L_g$ from Eqn.~(\ref{eqn:combined_L_g}).

~

\subsection{Improving Numerical Stability}

Due to the necessarily limited precision of numerical floating point representations~\cite{Goldberg_1991,Monniaux_2008},
the direct computation of Eqns.~(\ref{eqn:gmm_prob}) and (\ref{eqn:gaussian}) can quickly lead to numerical underflows or overflows,
which in turn lead to either poor models or a complete failure to estimate the parameters.
%To address this problem, the logarithm version of Eqn.~(\ref{eqn:gmm_prob}) can be used:
To address this problem, the following reformulation can be used.
First, logarithm version of Eqn.~(\ref{eqn:gaussian}) is taken:
%
\begin{multline}
  \log {{\mathcal{N}}}( \Vec{x} | \Vec{\mu}, \Mat{\Sigma} )
%   & = &
%   \log \left\{
%   \frac{1}{ (2\pi)^{\frac{D}{2}} | \Mat{\Sigma}|^{\frac{1}{2}} }
%   \exp \left[ -\frac{1}{2} (\Vec{x}-\Vec{\mu})^\top \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu}) \right]
%   \right\} \\
% %
%   & = &
%   \log \left\{
%   \frac{1}{ (2\pi)^{\frac{D}{2}} | \Mat{\Sigma}|^{\frac{1}{2}} } \right\}
%   + 
%   \log \left\{
%   \exp \left[ -\frac{1}{2} (\Vec{x}-\Vec{\mu})^\top \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu}) \right]
%   \right\} \\
% %
%   & = &
%   -  \log \left\{
%   (2\pi)^{\frac{D}{2}} | \Mat{\Sigma}|^{\frac{1}{2}} \right\}
%   + 
%   -\frac{1}{2} (\Vec{x}-\Vec{\mu})^\top \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu})
%   \\ 
% %
%   & = &
%   - \left(
%   \log \left\{ (2\pi)^{\frac{D}{2}} \right\} + \log \left\{  | \Mat{\Sigma}|^{\frac{1}{2}} \right\}
%   \right)
%   + 
%   -\frac{1}{2} (\Vec{x}-\Vec{\mu})^\top \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu})
%   \\
% %
%   & = &
%   - \left(
%   \frac{D}{2} \log \left( 2\pi \right)
%   +
%   \frac{1}{2} \log ( |\Mat{\Sigma}| )
%   \right)
%   -
%   \frac{1}{2} (\Vec{x}-\Vec{\mu})^\top \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu})
%   \\
% %
  = -\left\{\sfrac{D}{2} \log \left( 2\pi \right)
  +
  \sfrac{1}{2} ~ \log ( |\Mat{\Sigma}| )
  \right\} \\ - \sfrac{1}{2} (\Vec{x}-\Vec{\mu})^\top \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu}),
\end{multline}
%
\mbox{which leads to the corresponding logarithm version of Eqn.~(\ref{eqn:gmm_prob}):}

\begin{multline}
\log \sum\nolimits_{g=1}^{N_G} w_g ~ {{\mathcal{N}}}( \Vec{x} ~|~ \Vec{\mu}_g, \Mat{\Sigma}_g )
 = \\
 \log \sum\nolimits_{g=1}^{N_G} \exp\left[ \log \left\{ w_g {{\mathcal{N}}}( \Vec{x} ~|~ \Vec{\mu}_g, \Mat{\Sigma}_g ) \right\} \right].
\label{eqn:log_gmm}
\end{multline}

\noindent
The right hand side of Eqn.~(\ref{eqn:log_gmm}) can be expressed as a repeated addition in the form of:
%
\begin{equation}
\log\left( \exp\left[\log(a)\right] + \exp\left[\log(b)\right] \right),
\end{equation}%
%
which in turn can be rewritten in the form of:
%
\begin{equation}
\log(a) + \log\left( 1 + \exp\left[ \log(b) - \log(a) \right] \right).
\end{equation}%

\noindent
In the latter form, if we ensure that {$\log(a) \geq \log(b)$} (through swapping $\log(a)$ and $\log(b)$ when required),
the exponential will always produce values $\leq 1$ which helps to reduce the occurrence of overflows.
Overall, by keeping most of the computation in the {\it log} domain, 
both underflows and overflows are considerably reduced.

A further practical issue is the occurrence of \mbox{degenerate} or ill-conditioned covariance matrices,
stemming from either not enough samples with $l_{g,i} > 0$ contributing to the calculation of $\widehat{\Mat{\Sigma}}_g$ in Eqn.~(\ref{eqn:em_cov}),
or from too many samples which are essentially the same (ie.,~very low variance).
When the diagonal entries in a covariance matrix are too close to zero,
inversion of the matrix is unstable and can cause the calculated log-likelihood to become unreliable
or non-finite.
A straightforward and effective approach to address this problem is to place an artificial floor
on the diagonal entries in each covariance matrix after each EM iteration.
While the optimum value of the floor is data dependent, a small positive constant is typically sufficient to promote numerical stability
and convergence.

% Formally, the robust version of \mbox{\small $\log(\exp\left[\log(a)\right] + \exp\left[\log(b)\right])$} can be expressed as 
% $\operatorname{\Psi}(\log(a), \log(b))$ is used as 
% %
% \begin{eqnarray}
% \operatorname{\Psi}(\log(a), \log(b))
% & = & \left\{
% \begin{array}{ll}
% \operatorname{\Omega}(\log(a),\log(b)), & \mbox{if} ~ \log(a) \geq \log(b) \\
% \operatorname{\Omega}(\log(b),\log(a)), & \mbox{otherwise}
% \end{array}
% \right. \nonumber \\
% %
% \operatorname{\Omega}(\log(a), \log(b))
% & = & \left\{
% \begin{array}{ll}
% \log(a),                   & \mbox{if} ~ (\log(b)-\log(a)) < \operatorname{log\_floor} \mbox{~or~} (\log(b)-\log(a)) \notin \mathbb{R} \\
% \log(a) + \log(1 + \exp(\log(b)-\log(a))),  & \mbox{otherwise} 
% \end{array}
% \right.  \nonumber
% \end{eqnarray}
% %
% In the above,
% $\operatorname{log\_floor}$ is a machine dependent constant, equal to approximately
% $\log(10^{-323})$ when using double precision floating point values%
% \footnote
%   {
%   The exact value in the C++ implementation is equal to {\it log({std::numeric\_limits{\textless}T{\textgreater}::min()})},
%   where {\it T} represents a floating point type (ie., either {\it float} or {\it double}).
%   }%
% .


% Given the logarithm versions of Eqns.~(\ref{eqn:gmm_prob}) and (\ref{eqn:gaussian}),
% the logarithm version of Eqn.~(\ref{eqn:aposteriori}) can be expressed as:


