\section{Parameter Estimation via Expectation Maximisation}
\label{sec:parameter_estimation}

\noindent
The overall likelihood for a set of samples, $X=\{\Vec{x}_i\}_{i=1}^{N_V}$,
is found using $p(X | \lambda) = \prod\nolimits_{i=1}^{N_V} p(\Vec{x}_i | \lambda)$.
A~parameter set $\lambda$ that suitably models the underlying distribution of $X$ can be estimated using a particular instance of the Expectation Maximisation (EM) algorithm~\cite{Dempster77, McLachlan-2008, Moon96, Redner84}.
As its name suggests, the EM algorithm is comprised of iterating two steps: the {\it expectation} step, followed by the {\it maximisation} step.
GMM parameters generated by the previous iteration ($\lambda^{\mbox{\footnotesize old}}$) are used
by the current iteration to generate a new set of parameters ($\lambda^{\mbox{\footnotesize new}}$), such that:
%
\begin{eqnarray}
	p(X|\lambda^{\mbox{\footnotesize new}}) \geq p(X|\lambda^{\mbox{\footnotesize old}})
\end{eqnarray}
%
A direct implementation of the EM algorithm, specific to GMMs, has one iteration succinctly expressed as follows:
%
\begin{eqnarray}
  \mbox{for} ~g=1, \cdots, N_G: \nonumber \\
  \mbox{for} ~i=1, \cdots, N_V: \nonumber \\
  l_{g,i} & = & \frac{w_g ~ {{\mathcal{N}}}( \Vec{x}_i | \Vec{\mu}_g, \Mat{\Sigma}_g )}
  {\sum\nolimits_{k=1}^{N_G} w_k ~ {{\mathcal{N}}}( \Vec{x}_i | \Vec{\mu}_k, \Mat{\Sigma}_k )} \label{eqn:aposteriori}\\
\mbox{for} ~~ g  =  1, \cdots, N_G:  \nonumber \\
	L_g & = & \sum\nolimits_{i=1}^{N_V} l_{g,i} \label{eqn:L_k} \\
	\widehat{w}_g & = & \frac{L_g}{N_V} \label{eqn:m_k} \\
	\widehat{\Vec{\mu}}_g & = & \frac{1}{L_g} \sum\nolimits_{i=1}^{N_V} \Vec{x}_i ~ l_{g,i}  \label{eqn:em_mean} \\
	\widehat{\Mat{\Sigma}}_g & = & \frac{1}{L_g} \sum\nolimits_{i=1}^{N_V} (\Vec{x}_i - \widehat{\Vec{\mu}}_g)(\Vec{x}_i - \widehat{\Vec{\mu}}_g)^T l_{g,i} \label{eqn:em_covA} \\
	& = &  \frac{1}{L_g} \left[ \sum\nolimits_{i=1}^{N_V} \Vec{x}_i \Vec{x}_i^T l_{g,i} \right] - \widehat{\Vec{\mu}}_g \widehat{\Vec{\mu}}_g^T \label{eqn:em_covB}
\end{eqnarray}

\noindent
Once all $\widehat{w}_g, \widehat{\Vec{\mu}}_g, \widehat{\Mat{\Sigma}}_g$ are found, the parameters are updated,
ie., $\left\{ w_g, \Vec{\mu}_g, \Mat{\Sigma}_g \right\}_{g=1}^{N_G} ~ = ~ \left\{ \widehat{w}_g, \widehat{\Vec{\mu}}_g, \widehat{\Mat{\Sigma}}_g \right\}_{g=1}^{N_G}$,
and the iteration starts anew.
The process is typically repeated until the number of iterations has reached a pre-defined number,
and/or the increase in the overall likelihood after each iteration falls below a pre-defined threshold.

\noindent In Eqn.~(\ref{eqn:aposteriori}), $l_{g,i} \in [0,1]$ is the {a-posteriori} probability of Gaussian $g$ given $\Vec{x}_i$ and current parameters.
Thus the estimates $\widehat{\Vec{\mu}}_g$ and $\widehat{\Mat{\Sigma}}_g$ are weighted versions of the
sample mean and sample covariance, respectively.
Overall, the algorithm is a hill climbing procedure for maximising $p(X | \lambda)$.
While there are no guarantees that it will reach a global maximum, it is guaranteed to monotonically converge to a saddle point or a local maximum~\cite{Dempster77,Duda01,Mitchell97}.
The above implementation can also be interpreted as an unsupervised probabilistic clustering procedure,
with $N_G$ being the assumed number of clusters.
For a derivation of the EM algorithm for GMM parameters, the reader is directed to~\cite{Bilmes98,Redner84} or Appendix~\ref{app:em_algorithm}.


\subsection{Improving Numerical Stability}

Due to the necessarily limited precision of numerical floating point representations~\cite{Goldberg_1991,Monniaux_2008},
the direct computation of Eqns.~(\ref{eqn:gmm_prob}) and (\ref{eqn:aposteriori}) can quickly lead to numerical underflows or overflows.
%To address this problem, the logarithm version of Eqn.~(\ref{eqn:gmm_prob}) can be used:
To address this problem, the logarithm version of Eqn.~(\ref{eqn:gaussian}) can be used:
%
\begin{eqnarray}
  \log {{\mathcal{N}}}( \Vec{x} | \Vec{\mu}, \Mat{\Sigma} )
%   & = &
%   \log \left\{
%   \frac{1}{ (2\pi)^{\frac{D}{2}} | \Mat{\Sigma}|^{\frac{1}{2}} }
%   \exp \left[ -\frac{1}{2} (\Vec{x}-\Vec{\mu})^T \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu}) \right]
%   \right\} \\
% %
%   & = &
%   \log \left\{
%   \frac{1}{ (2\pi)^{\frac{D}{2}} | \Mat{\Sigma}|^{\frac{1}{2}} } \right\}
%   + 
%   \log \left\{
%   \exp \left[ -\frac{1}{2} (\Vec{x}-\Vec{\mu})^T \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu}) \right]
%   \right\} \\
% %
%   & = &
%   -  \log \left\{
%   (2\pi)^{\frac{D}{2}} | \Mat{\Sigma}|^{\frac{1}{2}} \right\}
%   + 
%   -\frac{1}{2} (\Vec{x}-\Vec{\mu})^T \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu})
%   \\ 
% %
%   & = &
%   - \left(
%   \log \left\{ (2\pi)^{\frac{D}{2}} \right\} + \log \left\{  | \Mat{\Sigma}|^{\frac{1}{2}} \right\}
%   \right)
%   + 
%   -\frac{1}{2} (\Vec{x}-\Vec{\mu})^T \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu})
%   \\
% %
%   & = &
%   - \left(
%   \frac{D}{2} \log \left( 2\pi \right)
%   +
%   \frac{1}{2} \log ( |\Mat{\Sigma}| )
%   \right)
%   -
%   \frac{1}{2} (\Vec{x}-\Vec{\mu})^T \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu})
%   \\
% %
  & = &
  - \left\{
  \sfrac{D}{2} ~ \log \left( 2\pi \right)
  +
  \sfrac{1}{2} ~ \log ( |\Mat{\Sigma}| )
  \right\}
  -
  \sfrac{1}{2} ~ (\Vec{x}-\Vec{\mu})^T \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu})
\end{eqnarray}

Furthermore, Eqn.~(\ref{eqn:gmm_prob}) is correspondingly changed to:
%
\begin{eqnarray}
\log \sum\nolimits_{g=1}^{N_G} w_g ~ {{\mathcal{N}}}( \Vec{x} ~|~ \Vec{\mu}_g, \Mat{\Sigma}_g )
& = &
\log \sum\nolimits_{g=1}^{N_G} \exp\left[ \log \left\{ w_g {{\mathcal{N}}}( \Vec{x} ~|~ \Vec{\mu}_g, \Mat{\Sigma}_g ) \right\} \right]
\label{eqn:log_gmm}
\end{eqnarray}
The right hand side of Eqn.~(\ref{eqn:log_gmm}) can be expressed as a repeated addition in the form of \mbox{\small $\log(\exp\left[\log(a)\right] + \exp\left[\log(b)\right])$},
which can be rewritten in the form of \mbox{\small $\log(a) + \log\left( 1 + \exp\left[ \log(b) - \log(a) \right] \right)$}.
In the latter form, if we ensure that {\small $\log(a) \geq \log(b)$,
the exponential will always produce values $\leq 1$ which helps to reduce the occurence of overflows.

% Formally, the robust version of \mbox{\small $\log(\exp\left[\log(a)\right] + \exp\left[\log(b)\right])$} can be expressed as 
% $\operatorname{\Psi}(\log(a), \log(b))$ is used as 
% %
% \begin{eqnarray}
% \operatorname{\Psi}(\log(a), \log(b))
% & = & \left\{
% \begin{array}{ll}
% \operatorname{\Omega}(\log(a),\log(b)), & \mbox{if} ~ \log(a) \geq \log(b) \\
% \operatorname{\Omega}(\log(b),\log(a)), & \mbox{otherwise}
% \end{array}
% \right. \nonumber \\
% %
% \operatorname{\Omega}(\log(a), \log(b))
% & = & \left\{
% \begin{array}{ll}
% \log(a),                   & \mbox{if} ~ (\log(b)-\log(a)) < \operatorname{log\_floor} \mbox{~or~} (\log(b)-\log(a)) \notin \mathbb{R} \\
% \log(a) + \log(1 + \exp(\log(b)-\log(a))),  & \mbox{otherwise} 
% \end{array}
% \right.  \nonumber
% \end{eqnarray}
% %
% In the above,
% $\operatorname{log\_floor}$ is a machine dependent constant, equal to approximately
% $\log(10^{-323})$ when using double precision floating point values%
% \footnote
%   {
%   The exact value in the C++ implementation is equal to {\it log({std::numeric\_limits{\textless}T{\textgreater}::min()})},
%   where {\it T} represents a floating point type (ie., either {\it float} or {\it double}).
%   }%
% .


\subsection{Reformulation for Parallel Execution}

TODO

