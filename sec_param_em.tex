\section{Parameter Estimation via Expectation Maximisation}
\label{sec:param_em}

\noindent
The overall likelihood for a set of samples, $X=\{\Vec{x}_i\}_{i=1}^{N_V}$,
is found using $p(X | \lambda) = \prod\nolimits_{i=1}^{N_V} p(\Vec{x}_i | \lambda)$.
A~parameter set $\lambda$ that suitably models the underlying distribution of $X$ can be estimated using a particular instance of the Expectation Maximisation (EM) algorithm~\cite{Dempster77, McLachlan-2008, Moon96, Redner84}.
As its name suggests, the EM algorithm is comprised of iterating two steps: the {\it expectation} step, followed by the {\it maximisation} step.
GMM parameters generated by the previous iteration ($\lambda^{\textrm{old}}$) are used
by the current iteration to generate a new set of parameters ($\lambda^{\textrm{new}}$), such that:
%
\begin{eqnarray}
	p(X|\lambda^{\textrm{new}}) \geq p(X|\lambda^{\textrm{old}})
\end{eqnarray}
%
In a direct implementation of the EM algorithm specific to GMMs,
the estimated versions of the parameters ($\widehat{w}_g$, $\widehat{\Vec{\mu}}_g$, $\widehat{\Mat{\Sigma}}_g$)
within one iteration are calculated as follows:
%
\begin{eqnarray}
  l_{g,i}                  & = & \frac{w_g ~ {{\mathcal{N}}}( \Vec{x}_i | \Vec{\mu}_g, \Mat{\Sigma}_g )}{\sum\nolimits_{k=1}^{N_G} w_k ~ {{\mathcal{N}}}( \Vec{x}_i | \Vec{\mu}_k, \Mat{\Sigma}_k )} \label{eqn:aposteriori} \\
  L_g                      & = & \sum\nolimits_{i=1}^{N_V} l_{g,i} \\
  \widehat{w}_g            & = & \frac{L_g}{N_V}  \label{eqn:em_weight} \\
  \widehat{\Vec{\mu}}_g    & = & \frac{1}{L_g} \sum\nolimits_{i=1}^{N_V} \Vec{x}_i ~ l_{g,i}  \label{eqn:em_mean} \\
  \widehat{\Mat{\Sigma}}_g & = & \frac{1}{L_g} \sum\nolimits_{i=1}^{N_V} (\Vec{x}_i - \widehat{\Vec{\mu}}_g)(\Vec{x}_i - \widehat{\Vec{\mu}}_g)^\top l_{g,i} \label{eqn:em_cov} \\
                           & = &  \frac{1}{L_g} \left[ \sum\nolimits_{i=1}^{N_V} \Vec{x}_i \Vec{x}_i^\top l_{g,i} \right] - \widehat{\Vec{\mu}}_g \widehat{\Vec{\mu}}_g^\top
\end{eqnarray}

\noindent
Once the estimated parameters for all Gaussians are found, the parameters are updated,
ie., {\small $\left\{ w_g, \Vec{\mu}_g, \Mat{\Sigma}_g \right\}_{g=1}^{N_G} = \left\{ \widehat{w}_g, \widehat{\Vec{\mu}}_g, \widehat{\Mat{\Sigma}}_g \right\}_{g=1}^{N_G}$},
and the iteration starts anew.
The process is typically repeated until the number of iterations has reached a pre-defined number,
and/or the increase in the overall likelihood after each iteration falls below a pre-defined threshold.

\noindent In Eqn.~(\ref{eqn:aposteriori}), $l_{g,i} \in [0,1]$ is the {a-posteriori} probability of Gaussian $g$ given $\Vec{x}_i$ and current parameters.
Thus the estimates $\widehat{\Vec{\mu}}_g$ and $\widehat{\Mat{\Sigma}}_g$ are weighted versions of the
sample mean and sample covariance, respectively.

Overall, the algorithm is a hill climbing procedure for maximising $p(X | \lambda)$.
While there are no guarantees that it will reach a global maximum, it is guaranteed to monotonically converge to a saddle point or a local maximum~\cite{Dempster77,Duda01,Mitchell97}.
The above implementation can also be interpreted as an unsupervised probabilistic clustering procedure,
with $N_G$ being the assumed number of clusters.
For a derivation of the EM algorithm tailored to GMMs, the reader is directed to~\cite{Bilmes98,Redner84} or Appendix~A. %~\ref{app:em_algorithm}.


\subsection{Reformulation for Parallel Execution}
\label{sec:param_em_parallel}

The EM algorithm is quite computationally intensive.
This is in large part due to the use of the $\exp(\cdot)$ function, which needs to be applied numerous times for each and every sample.
Fortunately, multi-core (multi-threaded) machines are now quite common and accessible, allowing for parallel execution.

One approach for parallelisation is the MapReduce framework~\cite{MapReduce_2004},
where data is split into chunks and farmed out to worker threads for processing (mapping).
The results are then collected and combined (reduced) to produce the final result.
Below we provide a reformulation of the EM algorithm into a MapReduce-like framework.

Given $N_T$ threads, the training samples are split into $N_T$ chunks (of approximately the same).
For thread with index $t \in [1,N_T]$, the start index of the samples is denoted by $i^{[t]}_{\textrm{start}}$,
while the end index is denoted by $i^{[t]}_{\textrm{end}}$.
For each thread $t$ and Gaussian $g \in [1,N_G]$, accumulators $\widetilde{L}_g^{[t]}$, $\widetilde{\Vec{\mu}}_g^{[t]}$ and $\widetilde{\Mat{\Sigma}}_g^{[t]}$
are calculated as follows:
%
\begin{eqnarray}
  %\mbox{for} ~~ g = 1, \cdots, N_g:  \nonumber \\
  %
  \widetilde{L}_g^{[t]}            & = & \sum\nolimits_{j = i^{[t]}_{\textrm{start}}}^{i^{[t]}_{\textrm{end}}} l_{g,j}                             \\ % acc_norm_lhoods
  \widetilde{\Vec{\mu}}_g^{[t]}    & = & \sum\nolimits_{j = i^{[t]}_{\textrm{start}}}^{i^{[t]}_{\textrm{end}}} l_{g,j} ~ \Vec{x}_j                 \\ % acc_mean
  %\widetilde{\Mat{\Sigma}}_g^{[t]} & = & \sum\nolimits_{j = i^{[t]}_{\textrm{start}}}^{i^{[t]}_{\textrm{end}}} l_{g,j} ~ \Vec{x}_j \odot \Vec{x}_j    % acc_dcov
  \widetilde{\Mat{\Sigma}}_g^{[t]} & = & \sum\nolimits_{j = i^{[t]}_{\textrm{start}}}^{i^{[t]}_{\textrm{end}}} l_{g,j} ~ \Vec{x}_j \Vec{x}_j^\top    % acc_dcov
\end{eqnarray}%
%
where $l_{g,j}$ is defined in Eqn.~(\ref{eqn:aposteriori}).
%while $\odot$ represents the Schur product (element-wise multiplication).

Once the accumulators for all threads are calculated,
for each Gaussian $g$ the reduction operation combines them to form the estimates of $\widehat{w}_g$, $\widehat{\Vec{\mu}}_g$ and $\widehat{\Mat{\Sigma}}_g$ as follows:
%
\begin{eqnarray}
  L_g                      & = & \sum\nolimits_{t=1}^{N_T} \widetilde{L}_g^{[t]}                           \\ % acc_norm_lhood
  \widehat{w}_g            & = & \frac{L_g}{N_V}                                                           \\
  \widehat{\Vec{\mu}}_g    & = & \frac{1}{L_g} \sum\nolimits_{t=1}^{N_T} \widetilde{\Vec{\mu}}_g^{[t]}     \\
  \widehat{\Mat{\Sigma}}_g & = & \frac{1}{L_g} \sum\nolimits_{t=1}^{N_T} \widetilde{\Mat{\Sigma}}_g^{[t]} - \widehat{\Vec{\mu}}_g \widehat{\Vec{\mu}}_g^\top
\end{eqnarray}

\subsection{Improving Numerical Stability}

Due to the necessarily limited precision of numerical floating point representations~\cite{Goldberg_1991,Monniaux_2008},
the direct computation of Eqns.~(\ref{eqn:gmm_prob}) and (\ref{eqn:gaussian}) can quickly lead to numerical underflows or overflows.
%To address this problem, the logarithm version of Eqn.~(\ref{eqn:gmm_prob}) can be used:
To address this problem, the logarithm version of Eqn.~(\ref{eqn:gaussian}) can be used:
%
\begin{eqnarray}
  \log {{\mathcal{N}}}( \Vec{x} | \Vec{\mu}, \Mat{\Sigma} )
%   & = &
%   \log \left\{
%   \frac{1}{ (2\pi)^{\frac{D}{2}} | \Mat{\Sigma}|^{\frac{1}{2}} }
%   \exp \left[ -\frac{1}{2} (\Vec{x}-\Vec{\mu})^\top \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu}) \right]
%   \right\} \\
% %
%   & = &
%   \log \left\{
%   \frac{1}{ (2\pi)^{\frac{D}{2}} | \Mat{\Sigma}|^{\frac{1}{2}} } \right\}
%   + 
%   \log \left\{
%   \exp \left[ -\frac{1}{2} (\Vec{x}-\Vec{\mu})^\top \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu}) \right]
%   \right\} \\
% %
%   & = &
%   -  \log \left\{
%   (2\pi)^{\frac{D}{2}} | \Mat{\Sigma}|^{\frac{1}{2}} \right\}
%   + 
%   -\frac{1}{2} (\Vec{x}-\Vec{\mu})^\top \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu})
%   \\ 
% %
%   & = &
%   - \left(
%   \log \left\{ (2\pi)^{\frac{D}{2}} \right\} + \log \left\{  | \Mat{\Sigma}|^{\frac{1}{2}} \right\}
%   \right)
%   + 
%   -\frac{1}{2} (\Vec{x}-\Vec{\mu})^\top \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu})
%   \\
% %
%   & = &
%   - \left(
%   \frac{D}{2} \log \left( 2\pi \right)
%   +
%   \frac{1}{2} \log ( |\Mat{\Sigma}| )
%   \right)
%   -
%   \frac{1}{2} (\Vec{x}-\Vec{\mu})^\top \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu})
%   \\
% %
  & = &
  - \left\{
  \sfrac{D}{2} ~ \log \left( 2\pi \right)
  +
  \sfrac{1}{2} ~ \log ( |\Mat{\Sigma}| )
  \right\}
  -
  \sfrac{1}{2} ~ (\Vec{x}-\Vec{\mu})^\top \Mat{\Sigma}^{-1} (\Vec{x}-\Vec{\mu})
\end{eqnarray}

The corresponding logarithm version of Eqn.~(\ref{eqn:gmm_prob}) is:
%
\begin{eqnarray}
\log \sum\nolimits_{g=1}^{N_G} w_g ~ {{\mathcal{N}}}( \Vec{x} ~|~ \Vec{\mu}_g, \Mat{\Sigma}_g )
& = &
\log \sum\nolimits_{g=1}^{N_G} \exp\left[ \log \left\{ w_g {{\mathcal{N}}}( \Vec{x} ~|~ \Vec{\mu}_g, \Mat{\Sigma}_g ) \right\} \right]
\label{eqn:log_gmm}
\end{eqnarray}
The right hand side of Eqn.~(\ref{eqn:log_gmm}) can be expressed as a repeated addition in the form of \mbox{\small $\log(\exp\left[\log(a)\right] + \exp\left[\log(b)\right])$},
which can be rewritten in the form of \mbox{\small $\log(a) + \log\left( 1 + \exp\left[ \log(b) - \log(a) \right] \right)$}.
In the latter form, if we ensure that {\small $\log(a) \geq \log(b)$},
the exponential will always produce values $\leq 1$ which helps to reduce the occurrence of overflows.

% Formally, the robust version of \mbox{\small $\log(\exp\left[\log(a)\right] + \exp\left[\log(b)\right])$} can be expressed as 
% $\operatorname{\Psi}(\log(a), \log(b))$ is used as 
% %
% \begin{eqnarray}
% \operatorname{\Psi}(\log(a), \log(b))
% & = & \left\{
% \begin{array}{ll}
% \operatorname{\Omega}(\log(a),\log(b)), & \mbox{if} ~ \log(a) \geq \log(b) \\
% \operatorname{\Omega}(\log(b),\log(a)), & \mbox{otherwise}
% \end{array}
% \right. \nonumber \\
% %
% \operatorname{\Omega}(\log(a), \log(b))
% & = & \left\{
% \begin{array}{ll}
% \log(a),                   & \mbox{if} ~ (\log(b)-\log(a)) < \operatorname{log\_floor} \mbox{~or~} (\log(b)-\log(a)) \notin \mathbb{R} \\
% \log(a) + \log(1 + \exp(\log(b)-\log(a))),  & \mbox{otherwise} 
% \end{array}
% \right.  \nonumber
% \end{eqnarray}
% %
% In the above,
% $\operatorname{log\_floor}$ is a machine dependent constant, equal to approximately
% $\log(10^{-323})$ when using double precision floating point values%
% \footnote
%   {
%   The exact value in the C++ implementation is equal to {\it log({std::numeric\_limits{\textless}T{\textgreater}::min()})},
%   where {\it T} represents a floating point type (ie., either {\it float} or {\it double}).
%   }%
% .


% Given the logarithm versions of Eqns.~(\ref{eqn:gmm_prob}) and (\ref{eqn:gaussian}),
% the logarithm version of Eqn.~(\ref{eqn:aposteriori}) can be expressed as:


