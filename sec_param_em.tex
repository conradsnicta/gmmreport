\section{Parameter Estimation via Expectation Maximisation}
\label{sec:parameter_estimation}

\noindent
The overall likelihood for a set of samples, $X=\{\Vec{x}_i\}_{i=1}^{N_V}$,
is found using $p(X | \lambda) = \prod\nolimits_{i=1}^{N_V} p(\Vec{x}_i | \lambda)$.
A~parameter set $\lambda$ that suitably models the underlying distribution of $X$ can be estimated using a particular instance of the Expectation Maximisation (EM) algorithm~\cite{Dempster77, McLachlan-2008, Moon96, Redner84}.
As its name suggests, the EM algorithm is comprised of iterating two steps: the {\it expectation} step, followed by the {\it maximisation} step.
GMM parameters generated by the previous iteration ($\lambda^{\mbox{\footnotesize old}}$) are used
by the current iteration to generate a new set of parameters ($\lambda^{\mbox{\footnotesize new}}$), such that:
%
\begin{eqnarray}
	p(X|\lambda^{\mbox{\footnotesize new}}) \geq p(X|\lambda^{\mbox{\footnotesize old}})
\end{eqnarray}
%
A direct implementation of the EM algorithm, specific to GMMs, has one iteration succinctly expressed as follows:
%
\begin{eqnarray}
  \mbox{for} ~g=1, \cdots, N_G: \nonumber \\
  \mbox{for} ~i=1, \cdots, N_V: \nonumber \\
  l_{g,i} & = & \frac{w_g ~ {{\mathcal{N}}}( \Vec{x}_i | \Vec{\mu}_g, \Mat{\Sigma}_g )}
  {\sum\nolimits_{k=1}^{N_G} w_k ~ {{\mathcal{N}}}( \Vec{x}_i | \Vec{\mu}_k, \Mat{\Sigma}_k )} \label{eqn:aposteriori}\\
\mbox{for} ~~ g  =  1, \cdots, N_G:  \nonumber \\
	L_g & = & \sum\nolimits_{i=1}^{N_V} l_{g,i} \label{eqn:L_k} \\
	\widehat{w}_g & = & \frac{L_g}{N_V} \label{eqn:m_k} \\
	\widehat{\Vec{\mu}}_g & = & \frac{1}{L_g} \sum\nolimits_{i=1}^{N_V} \Vec{x}_i ~ l_{g,i}  \label{eqn:em_mean} \\
	\widehat{\Mat{\Sigma}}_g & = & \frac{1}{L_g} \sum\nolimits_{i=1}^{N_V} (\Vec{x}_i - \widehat{\Vec{\mu}}_g)(\Vec{x}_i - \widehat{\Vec{\mu}}_g)^T l_{g,i} \label{eqn:em_covA} \\
	& = &  \frac{1}{L_g} \left[ \sum\nolimits_{i=1}^{N_V} \Vec{x}_i \Vec{x}_i^T l_{g,i} \right] - \widehat{\Vec{\mu}}_g \widehat{\Vec{\mu}}_g^T \label{eqn:em_covB}
\end{eqnarray}

\noindent
Once all $\widehat{w}_g, \widehat{\Vec{\mu}}_g, \widehat{\Mat{\Sigma}}_g$ are found, the parameters are updated,
ie., $\left\{ w_g, \Vec{\mu}_g, \Mat{\Sigma}_g \right\}_{g=1}^{N_G} ~ = ~ \left\{ \widehat{w}_g, \widehat{\Vec{\mu}}_g, \widehat{\Mat{\Sigma}}_g \right\}_{g=1}^{N_G}$,
and the iteration starts anew.
The process is typically repeated until the number of iterations has reached a pre-defined number,
and/or the increase in the overall likelihood after each iteration falls below a pre-defined threshold.

\noindent In Eqn.~(\ref{eqn:aposteriori}), $l_{g,i} \in [0,1]$ is the {a-posteriori} probability of Gaussian $g$ given $\Vec{x}_i$ and current parameters.
Thus the estimates $\widehat{\Vec{\mu}}_g$ and $\widehat{\Mat{\Sigma}}_g$ are weighted versions of the
sample mean and sample covariance, respectively.
Overall, the algorithm is a hill climbing procedure for maximising $p(X | \lambda)$.
While there are no guarantees that it will reach a global maximum, it is guaranteed to monotonically converge to a saddle point or a local maximum~\cite{Dempster77,Duda01,Mitchell97}.
The above implementation can also be interpreted as an unsupervised probabilistic clustering procedure,
with $N_G$ being the assumed number of clusters.
For a derivation of the EM algorithm for GMM parameters, the reader is directed to~\cite{Bilmes98,Redner84} or Appendix~\ref{app:em_algorithm}.


\subsection{Reformulation for Parallel Execution}

TODO
