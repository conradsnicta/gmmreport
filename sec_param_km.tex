\vspace{4ex}
\section{Initialisation via Multi-Threaded {\it k}-Means}
\label{sec:param_km}
\vspace{1ex}

As a starting point, the initial means can be set to randomly selected training vectors,
the initial covariance matrices can be set equal to identity matrices, 
and the initial weights can be uniform.
However, the $\exp(\cdot)$ function as well as the matrix inverse in Eqn.~(\ref{eqn:gaussian}) are typically quite time consuming to compute.
In order to speed up training, the initial estimate of $\lambda$ is typically provided via the {\it k}-means clustering algorithm~\cite{Bishop_2006,Duda01,Kulis_2012}
which avoids such time consuming operations.

The baseline {\it k}-means clustering algorithm is a straightforward iterative procedure comprised of two steps:
(i)~calculating the distance from each sample to each mean,
and
(ii)~calculating the new version of each mean as the average of samples which were found to be the closest to the previous version of the corresponding mean.
The required number of iterations is data dependent,
but about 10 iterations are often sufficient to generate a good initial estimate of $\lambda$.

The $k$-means algorithm can be interpreted as a simplified version (or special case) of the EM algorithm for GMMs~\cite{Kulis_2012}.
Instead of each sample being assigned a set probabilities representing cluster membership (soft assignment),
each sample is assigned to only one cluster (hard assignment).
Furthermore, it can be assumed that the covariance matrix of each Gaussian is non-informative, diagonal, and/or shared across all Gaussians.
More formally, the estimation of model parameters is as per Eqns.~(\ref{eqn:em_weight}), (\ref{eqn:em_mean}) and (\ref{eqn:em_cov}), 
but $l_{g,i}$ is redefined~as:%

\begin{equation}
  l_{g,i} = \left\{
  \begin{array}{ll}
  1, & \mbox{if} ~ g = \argmin\limits_{k=1, \cdots, N_G} \operatorname{dist}(\Vec{\mu}_k, \Vec{x}_i) \\
  0, & \mbox{otherwise}.
  \end{array}
  \right.
  \label{eqn:binary_likelihood}
\end{equation}
%
where {$\operatorname{dist}(\Vec{a}, \Vec{b})$} is a distance metric.
Apart from this difference, the parameter estimation is the same as for EM.
As such, multi-threading is achieved as per Section~\ref{sec:param_em_parallel}.

We note that it is possible to implement the \mbox{{\it k}-means} algorithm is a multitude of ways,
such as the cluster splitting LBG algorithm~\cite{Linde80},
or use an elaborate strategy for selecting the initial means~\cite{Arthur_2007}.
While there are also alternative and more complex implementations offering relatively fast execution~\cite{Elkan_2003},
we have elected to adapt the baseline \mbox{{\it k}-means} algorithm due to its straightforward amenability to multi-threading. 

% The vast majority of the computational effort for training GMMs is typically taken up by the full EM algorithm (as described in Section~\ref{sec:param_em}),
% hence the speed of the {\it k}-means algorithm is not critical in the context of GMMs.

\vspace{1ex}
\subsection{Issues with Modelling Accuracy and Convergence}

A typical and naive choice for the distance in~Eqn.~(\ref{eqn:binary_likelihood})
is the squared Euclidean distance, \mbox{$\operatorname{dist}(\Vec{a}, \Vec{b}) = \| \Vec{a} - \Vec{b} \|^{2}_{2}$}.
However, for multivariate datasets formed by combining data from various sensors, there is a notable downside to using the Euclidean distance.
When one of the dimensions within the data has a much larger range than the other dimensions,
it will dominate the contribution to the overall distance, with the other dimensions effectively ignored.
This can adversely skew the initial parameter estimates, easily leading to poor initial conditions for the EM algorithm.
This in turn can lead to poor modelling, as the EM algorithm is only guaranteed to reach a local maximum~\cite{Dempster77,Duda01,Mitchell97}.
To address this problem, the squared Mahalanobis distance can be used~\cite{Bishop_2006,Duda01}:

\begin{equation}
\operatorname{dist}(\Vec{a}, \Vec{b}) = (\Vec{a} - \Vec{b})^\top \Mat{\Sigma}^{-1}_{\mathrm{global}} (\Vec{a} - \Vec{b}),
\end{equation}
\noindent where $\Mat{\Sigma}_{\mathrm{global}}$ is a global covariance matrix, estimated from all available training data.
To maintain efficiency, $\Mat{\Sigma}_{\mathrm{global}}$ is typically diagonal,
which makes calculating its inverse straightforward (ie., reciprocals of the values on the main diagonal).



In practice it is possible that while iterating at least one of the means has no vectors assigned to it,
becoming a ``dead'' mean.
This might stem from an unfortunate starting point, 
or specifying a relatively large value for $N_G$ for modelling a relatively small dataset.
As such, an additional heuristic is required to attempt to resolve this situation.
An effective approach for resurrecting a ``dead'' mean is to make it equal to one of the vectors
that has been assigned to the most ``popular'' mean,
where the most ``popular'' mean is the mean that currently has the most vectors assigned to it.



% The pseudo-code for a direct implementation of a baseline $k$-means algorithm is shown in Figure~\ref{fig:kmeans_pseudocode}.
% 
% \begin{figure}[!b]
% \hrule
% \vspace{0.5ex}
% \begin{small}
% \begin{tabbing}
% 01: {\bf for} $g=1, ~\cdots, ~N_G$ \\
% 02: ~~ $\Vec{\mu}_g = \Vec{x}_{ \mbox{\it randi}(1, N_V) } $ ~~~ {\small // randomly select initial means} \\
% 03: {\bf endfor} \\
% 04: {\it iteration} = $1$ \\
% 05: {\it final\_iteration} = $10$ ~~~ ~~~ ~~~ {\small // empirically chosen termination condition} \\
% 06: {\it finished} = FALSE \\
% 07: {\bf while} {\it finished} $\neq$ TRUE \\
% 08: ~~ {\bf for} $i=1, ~\cdots, ~N_V$ \\
% 09: ~~ ~~ \( y_i = {\displaystyle \arg \min_{g=1, \cdots, N_G}} \operatorname{\mbox{\it dist}}(\Vec{\mu}_g, \Vec{x}_i) \) ~~~ {\small // label each vector as belonging to its closest mean} \\
% 10: ~~ {\bf endfor} \\
% 11: ~~ {\bf for} $g=1, ~\cdots, ~N_G$ \\
% 12: ~~ ~~ $n_g = \sum\nolimits_{i=1}^{N_V} \delta(y_i, g) $  ~~~ ~~~ ~~~ ~~~ ~ {\small // count the number of vectors assigned to each mean} \\
% 13: ~~ ~~ $\widehat{\Vec{\mu}}_g = \frac{1}{n_g} \sum\nolimits_{i=1}^{N_V} \Vec{x}_i \delta(y_i, g) $ ~~~ ~~~ {\small // find the new mean using vectors assigned to the old mean} \\
% 14: ~~ {\bf endfor} \\
% 15: ~~ {\it same} = TRUE \\
% 16: ~~ {\bf for} $g=1, ~\cdots, ~N_G$ \\
% 17: ~~ ~~ {\bf if} $\widehat{\Vec{\mu}}_g \neq \Vec{\mu}_g $  ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~~~ {\small // see if the means have changed since the last iteration} \\
% 18: ~~ ~~ ~~ {\it same} = FALSE \\
% 19: ~~ ~~ {\bf endif} \\
% 20: ~~ {\bf endfor} \\
% 21: ~~ {\bf for} $g=1, ~\cdots, ~N_G$ \\
% 22: ~~ ~~ $\Vec{\mu}_g = \widehat{\Vec{\mu}}_g$ ~~~ {\small // update the mean vectors} \\
% 23: ~~ {\bf endfor} \\
% 24: ~~ {\it iteration} = {\it iteration} + 1 \\
% 25: ~~ {\bf if} ({\it same} == TRUE) {\bf or} ({\it iteration} $>$ {\it final\_iteration})  \\
% 26: ~~ ~~ {\it finished} = TRUE \\
% 27: ~~ {\bf endif} \\
% 28: {\bf endwhile}
% \end{tabbing}
% \end{small}
% \vspace{-1ex}
% \hrule
% \caption
%   {
%   \small
%   \TODO{change to normal equations}.
%   Pseudo-code for a direct implementation of the classic $k$-means algorithm.
%   Each training vector is denoted as $\Vec{x}_i$, where $i = 1, \cdots, N_V$, with $N_V$ indicating the number of vectors available for training.
%   Each mean vector is denoted as $\Vec{\mu}_g$, where $g = 1, \cdots, N_G$, with $N_G$ indicating the number of required means (centroids).
%   The~{\it randi}$(min,max)$ function generates a uniformly distributed random integer value in the $[min,max]$ interval.
%   The~{\it dist}$(\Vec{x},\Vec{y})$ function calculates a distance between vectors $\Vec{x}$ and $\Vec{y}$, with a distance of zero indicating that the vectors are equal.
%   The~$\delta(\cdot,\cdot)$ function (Kronecker delta) is equal to either $1$ or $0$, corresponding to its two arguments either matching or not matching.
%   }
% \label{fig:kmeans_pseudocode}
% \end{figure}

% Once the estimated means $\{ \Vec{\mu}_g \}_{g=1}^{N_G}$ have been found,
% the initial weights $\{ w_g \}_{g=1}^{N_G}$ and initial covariance matrices $\{ {\Mat{\Sigma}_g} \}_{g=1}^{N_G}$
% are estimated as follows:
% %
% \begin{eqnarray}
% w_g & = & \frac{n_g}{N_V} \\
% {\mbox {\boldmath $\Sigma$}_g} & = & \frac{1}{n_g} \sum\nolimits_{i=1}^{N_V} (\Vec{x}_i - \Vec{\mu}_g)(\Vec{x}_i - \Vec{\mu}_g)^T \delta(y_i, g)
% \label{eqn:k-means_cov}
% \end{eqnarray}
% %
% \noindent
% where $n_g$ is defined on line 12 of the pseudo-code and $y_i$ on line 9.
