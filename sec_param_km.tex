\section{Initial Estimatation via {\it k}-Means}
\label{sec:initial_estimate}

As a starting point, the initial means can be set to randomly selected training vectors,
the initial covariance matrices can be set equal to identity matrices, 
and the initial weights can be uniform.
However, in a computational implementation, the $\exp()$ function as well as the matrix inverse in Eqn.~(\ref{eqn:gaussian}) are typically quite time consuming.
In order to speed up training, the initial estimate of $\lambda$ is typically provided via the {\it k}-means clustering algorithm~\cite{Bishop_2006,Duda01,Kulis_2012}
which avoids such time consuming operations.

The baseline {\it k}-means clustering algorithm is a simple iterative procedure comprised of two steps:
(i)~calculating the Euclidean distance from each vector to each each mean,
and
(ii)~calculating the new version of each mean as the average of vectors which were found to be the closest to the previous version of the corresponding mean.
The pseudo-code for a direct implementation of a baseline $k$-means algorithm is shown in Figure~\ref{fig:kmeans_pseudocode}.

The $k$-means algorithm can be interpreted as a simplified version (or special case) of the EM algorithm for GMMs~\cite{Kulis_2012}.
Instead of each vector being assigned a set probabilities representing cluster membership (soft assignment),
each vector is assigned to only one cluster (hard assignment).
Furthermore, it can be assumed that the covariance matrix of each Gaussian is non-informative, diagonal, and/or shared across all Gaussians.

It is possible to implement the $k$-means algorithm is a multitude of ways,
such as the cluster splitting LBG algorithm~\cite{Linde80},
or use an elaborate strategy for selecting the initial means~\cite{Arthur_2007}.
There are also alternative implementations offering faster convergence~\cite{TODO,TODO}.
However, the vast majority of the computational effort for training GMMs is typically taken up by the EM algorithm,
and hence the speed of the {\it k}-means algorithm is not critical in the context of GMMs.

\begin{figure}[!b]
\hrule
\vspace{0.5ex}
\begin{small}
\begin{tabbing}
01: {\bf for} $g=1, ~\cdots, ~N_G$ \\
02: ~~ $\Vec{\mu}_g = \Vec{x}_{ \mbox{\it randi}(1, N_V) } $ ~~~ {\small // randomly select initial means} \\
03: {\bf endfor} \\
04: {\it iteration} = $1$ \\
05: {\it final\_iteration} = $10$ ~~~ ~~~ ~~~ {\small // empirically chosen termination condition} \\
06: {\it finished} = FALSE \\
07: {\bf while} {\it finished} $\neq$ TRUE \\
08: ~~ {\bf for} $i=1, ~\cdots, ~N_V$ \\
09: ~~ ~~ \( y_i = {\displaystyle \arg \min_{g=1, \cdots, N_G}} \operatorname{\mbox{\it dist}}(\Vec{\mu}_g, \Vec{x}_i) \) ~~~ {\small // label each vector as belonging to its closest mean} \\
10: ~~ {\bf endfor} \\
11: ~~ {\bf for} $g=1, ~\cdots, ~N_G$ \\
12: ~~ ~~ $n_g = \sum\nolimits_{i=1}^{N_V} \delta(y_i, g) $  ~~~ ~~~ ~~~ ~~~ ~ {\small // count the number of vectors assigned to each mean} \\
13: ~~ ~~ $\widehat{\Vec{\mu}}_g = \frac{1}{n_g} \sum\nolimits_{i=1}^{N_V} \Vec{x}_i \delta(y_i, g) $ ~~~ ~~~ {\small // find the new mean using vectors assigned to the old mean} \\
14: ~~ {\bf endfor} \\
15: ~~ {\it same} = TRUE \\
16: ~~ {\bf for} $g=1, ~\cdots, ~N_G$ \\
17: ~~ ~~ {\bf if} $\widehat{\Vec{\mu}}_g \neq \Vec{\mu}_g $  ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~~~ {\small // see if the means have changed since the last iteration} \\
18: ~~ ~~ ~~ {\it same} = FALSE \\
19: ~~ ~~ {\bf endif} \\
20: ~~ {\bf endfor} \\
21: ~~ {\bf for} $g=1, ~\cdots, ~N_G$ \\
22: ~~ ~~ $\Vec{\mu}_g = \widehat{\Vec{\mu}}_g$ ~~~ {\small // update the mean vectors} \\
23: ~~ {\bf endfor} \\
24: ~~ {\it iteration} = {\it iteration} + 1 \\
25: ~~ {\bf if} ({\it same} == TRUE) {\bf or} ({\it iteration} $>$ {\it final\_iteration})  \\
26: ~~ ~~ {\it finished} = TRUE \\
27: ~~ {\bf endif} \\
28: {\bf endwhile}
\end{tabbing}
\end{small}
\vspace{-1ex}
\hrule
\caption
  {
  \small
  \TODO{change to normal equations}.
  Pseudo-code for a direct implementation of the classic $k$-means algorithm.
  Each training vector is denoted as $\Vec{x}_i$, where $i = 1, \cdots, N_V$, with $N_V$ indicating the number of vectors available for training.
  Each mean vector is denoted as $\Vec{\mu}_g$, where $g = 1, \cdots, N_G$, with $N_G$ indicating the number of required means (centroids).
  The~{\it randi}$(min,max)$ function generates a uniformly distributed random integer value in the $[min,max]$ interval.
  The~{\it dist}$(\Vec{x},\Vec{y})$ function calculates a distance between vectors $\Vec{x}$ and $\Vec{y}$, with a distance of zero indicating that the vectors are equal.
  The~$\delta(\cdot,\cdot)$ function (Kronecker delta) is equal to either $1$ or $0$, corresponding to its two arguments either matching or not matching.
  }
\label{fig:kmeans_pseudocode}
\end{figure}

Once the estimated means $\{ \Vec{\mu}_g \}_{g=1}^{N_G}$ have been found,
the initial weights $\{ w_g \}_{g=1}^{N_G}$ and initial covariance matrices $\{ {\Mat{\Sigma}_g} \}_{g=1}^{N_G}$
are estimated as follows:
%
\begin{eqnarray}
w_g & = & \frac{n_g}{N_V} \\
{\mbox {\boldmath $\Sigma$}_g} & = & \frac{1}{n_g} \sum\nolimits_{i=1}^{N_V} (\Vec{x}_i - \Vec{\mu}_g)(\Vec{x}_i - \Vec{\mu}_g)^T \delta(y_i, g)
\label{eqn:k-means_cov}
\end{eqnarray}
%
\noindent
where $n_g$ is defined on line 12 of the pseudo-code and $y_i$ on line 9.


\subsection{Reformulation for Parallel Execution}

TODO


\subsection{Addressing Corner Cases}

In practise it is possible that while iterating at least one of the means has no vectors assigned to it,
becoming a ``dead'' mean.
This can be due to an unfortunate starting point and/or a lack of data.
As such, an additional heuristic is required to attempt to resolve this situation.
One example of resurrecting a ``dead'' mean is to make it equal to one of the vectors
that has been assigned to the most ``popular'' mean,
where the most ``popular'' mean is the mean that currently has the most vectors assigned to it.

